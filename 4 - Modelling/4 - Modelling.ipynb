{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b21671c9-6cbc-4fd9-8166-cff5fa249c08",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"display: flex; align-items: center; justify-content: center; padding: 20px 0; text-align: center; height: 100vh; max-width: 1500px; margin: auto;\">\n",
    "    <img src=\"https://media.licdn.com/dms/image/v2/D4D3DAQFGx0XnuUvugA/image-scale_191_1128/image-scale_191_1128/0/1662458005755/nova_ims_information_management_school_cover?e=2147483647&v=beta&t=J3Q4LlZi36_4UAFhj2019QdtfXLn0kQwaX25jgaBhOQ\" \n",
    "         alt=\"Logo\" \n",
    "         style=\"width: 100%; max-width: 1500px; height: auto; max-height: 200px; object-fit: cover; object-position: center; border: 5px solid #A0C020; border-radius: 5px;\">\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center; padding: 20px 0;\">\n",
    "    <h5>This analysis is conducted as part of the <b><i>Text Mining</i></b> course, a component of the Master's program in Data Science and Advanced Analytics at the <b><u>Nova Information Management School</u></b>.</h5>\n",
    "</div>\n",
    "<!-- This notebook template was created by Catarina GonÃ§alves Nunes, 20230083 -->\n",
    "<div style=\"text-align: center; color: #A0C020;\">\n",
    "    <h1><b>Predicting market behavior from tweets</b></h1>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24075c93-3716-4700-9c73-360ac71a35d7",
   "metadata": {},
   "source": [
    "<h3 style=\"text-align: center;\"><u></b>Group 31</u></b></h3>\n",
    "\n",
    "|     Student Name     |     Student ID     | \n",
    "|         ---          |           ---          |\n",
    "|     David|         | \n",
    "|     Elcano           |              |\n",
    "|     Jorge Cordeiro      |       20240594       |\n",
    "|     Rui   |            |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b0be95-ad99-48c0-ac2a-0c87a3772781",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "889e5757-3b29-456e-8c02-73d51bba5501",
   "metadata": {},
   "outputs": [],
   "source": [
    "# general and load_data.py functions\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from load_data import *\n",
    "from visualization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b0fc08d-440f-4469-9c36-f827203b482d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#specific model functions are below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c19254cb-30c3-49fb-9c49-adad5195ee49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4ed991-a245-44de-9275-7d75197a6deb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# LOGISTIC REGRESSION -delete this for using other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaefc747-9aac-49a0-8a0c-a24fb788bb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_simple_logreg(preprocessing_version, feature_type):\n",
    "  \n",
    "    print(f\"Training LogisticRegression with {preprocessing_version}/{feature_type} features\")\n",
    "    \n",
    "    # Load data\n",
    "    data = load_data(preprocessing_version, feature_type)\n",
    "    \n",
    "    X_train = data['X_train']\n",
    "    y_train = data['y_train']\n",
    "    X_val = data['X_val']\n",
    "    y_val = data['y_val']\n",
    "    X_test = data['X_test']\n",
    "    \n",
    "    # Create and train LogisticRegression model\n",
    "    logreg = LogisticRegression(C=1.0, penalty='l2', solver='lbfgs', max_iter=1000,class_weight='balanced',n_jobs=-1,random_state=42)\n",
    "    logreg.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_train_pred = logreg.predict(X_train)\n",
    "    y_val_pred = logreg.predict(X_val)\n",
    "    y_test_pred = logreg.predict(X_test)\n",
    "    \n",
    "    # Get prediction probabilities\n",
    "    y_val_prob = logreg.predict_proba(X_val)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    train_accuracy = accuracy_score(y_train, y_train_pred)\n",
    "    train_f1 = f1_score(y_train, y_train_pred, average='macro')\n",
    "    val_accuracy = accuracy_score(y_val, y_val_pred)\n",
    "    val_f1 = f1_score(y_val, y_val_pred, average='macro')\n",
    "    \n",
    "    # Calculate overfitting\n",
    "    overfitting = train_f1 - val_f1\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"Train Accuracy: {train_accuracy:.4f}, Train F1: {train_f1:.4f}\")\n",
    "    print(f\"Val Accuracy: {val_accuracy:.4f}, Val F1: {val_f1:.4f}\")\n",
    "    print(f\"Overfitting: {overfitting:.4f}\")\n",
    "    \n",
    "    # Create output directory\n",
    "    model_dir = os.path.join(\"model_results\", preprocessing_version, feature_type, \"logreg\")\n",
    "    os.makedirs(model_dir, exist_ok=True)\n",
    "    \n",
    "    # Use visualization functions\n",
    "    plot_confusion_matrix(y_val, y_val_pred, title=f'Confusion Matrix - LogReg ({preprocessing_version}, {feature_type})',output_path=os.path.join(model_dir, 'confusion_matrix.png'))\n",
    "    \n",
    "    plot_roc_curves(y_val, y_val_prob,class_names=[\"Bearish\", \"Bullish\", \"Neutral\"],title=f'ROC Curves - LogReg ({preprocessing_version}, {feature_type})',output_path=os.path.join(model_dir, 'roc_curves.png'))\n",
    "    \n",
    "    # Return results dictionary\n",
    "    return {'model': logreg, 'train_accuracy': train_accuracy, 'train_f1': train_f1, 'val_accuracy': val_accuracy, 'val_f1': val_f1, 'overfitting': overfitting, 'predictions': {'train': y_train_pred, 'val': y_val_pred, 'test': y_test_pred}, 'probabilities': {'val': y_val_prob}, 'params': {'C': 1.0, 'penalty': 'l2', 'solver': 'lbfgs', 'max_iter': 1000, 'class_weight': 'balanced'}}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809955a8-6f70-4516-b858-330849cd1ff4",
   "metadata": {},
   "source": [
    "# Evaluation of all the combinations - you might need to adapt these 2 cells below in terms of the names of the csv that are being exported"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33262bba-b622-495c-bf77-2d0ea47c3d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing LogisticRegression on all 15 combinations...\n",
      "\n",
      "[1/15] Processing regexp_snowball with tfidf\n",
      "Training LogisticRegression with regexp_snowball/tfidf features\n",
      "Loading data for regexp_snowball with tfidf features\n",
      "Loaded processed text data - Train: 8111, Val: 1432, Test: 2388\n",
      "Feature info: {'tfidf_shape': '(8111, 5000)', 'word2vec_shape': '(8111, 100)', 'mini_sbert_shape': '(8111, 384)', 'train_samples': '8111', 'val_samples': '1432', 'test_samples': '2388', 'class_weights': '{0: 2.2052746057640022, 1: 1.6546307629538963, 2: 0.5148860534501365}'}\n",
      "Loaded tfidf features - Train: (8111, 5000), Val: (1432, 5000), Test: (2388, 5000)\n",
      "Train Accuracy: 0.8947, Train F1: 0.8706\n",
      "Val Accuracy: 0.7884, Val F1: 0.7293\n",
      "Overfitting: 0.1413\n",
      "Results saved to: model_results\\regexp_snowball\\tfidf\\logreg_simple\n",
      "\n",
      "[2/15] Processing regexp_snowball with word2vec\n",
      "Training LogisticRegression with regexp_snowball/word2vec features\n",
      "Loading data for regexp_snowball with word2vec features\n",
      "Loaded processed text data - Train: 8111, Val: 1432, Test: 2388\n",
      "Feature info: {'tfidf_shape': '(8111, 5000)', 'word2vec_shape': '(8111, 100)', 'mini_sbert_shape': '(8111, 384)', 'train_samples': '8111', 'val_samples': '1432', 'test_samples': '2388', 'class_weights': '{0: 2.2052746057640022, 1: 1.6546307629538963, 2: 0.5148860534501365}'}\n",
      "Loaded word2vec features - Train: (8111, 100), Val: (1432, 100), Test: (2388, 100)\n",
      "Train Accuracy: 0.6107, Train F1: 0.5069\n",
      "Val Accuracy: 0.5859, Val F1: 0.4766\n",
      "Overfitting: 0.0303\n",
      "Results saved to: model_results\\regexp_snowball\\word2vec\\logreg_simple\n",
      "\n",
      "[3/15] Processing regexp_snowball with mini_sbert\n",
      "Training LogisticRegression with regexp_snowball/mini_sbert features\n",
      "Loading data for regexp_snowball with mini_sbert features\n",
      "Loaded processed text data - Train: 8111, Val: 1432, Test: 2388\n",
      "Feature info: {'tfidf_shape': '(8111, 5000)', 'word2vec_shape': '(8111, 100)', 'mini_sbert_shape': '(8111, 384)', 'train_samples': '8111', 'val_samples': '1432', 'test_samples': '2388', 'class_weights': '{0: 2.2052746057640022, 1: 1.6546307629538963, 2: 0.5148860534501365}'}\n",
      "Loaded mini_sbert features - Train: (8111, 384), Val: (1432, 384), Test: (2388, 384)\n",
      "Train Accuracy: 0.7011, Train F1: 0.6557\n",
      "Val Accuracy: 0.6425, Val F1: 0.5850\n",
      "Overfitting: 0.0706\n",
      "Results saved to: model_results\\regexp_snowball\\mini_sbert\\logreg_simple\n",
      "\n",
      "[4/15] Processing tweet_base with tfidf\n",
      "Training LogisticRegression with tweet_base/tfidf features\n",
      "Loading data for tweet_base with tfidf features\n",
      "Loaded processed text data - Train: 8111, Val: 1432, Test: 2388\n",
      "Feature info: {'tfidf_shape': '(8111, 5000)', 'word2vec_shape': '(8111, 100)', 'mini_sbert_shape': '(8111, 384)', 'train_samples': '8111', 'val_samples': '1432', 'test_samples': '2388', 'class_weights': '{0: 2.2052746057640022, 1: 1.6546307629538963, 2: 0.5148860534501365}'}\n",
      "Loaded tfidf features - Train: (8111, 5000), Val: (1432, 5000), Test: (2388, 5000)\n",
      "Train Accuracy: 0.8962, Train F1: 0.8721\n",
      "Val Accuracy: 0.7703, Val F1: 0.7024\n",
      "Overfitting: 0.1697\n",
      "Results saved to: model_results\\tweet_base\\tfidf\\logreg_simple\n",
      "\n",
      "[5/15] Processing tweet_base with word2vec\n",
      "Training LogisticRegression with tweet_base/word2vec features\n",
      "Loading data for tweet_base with word2vec features\n",
      "Loaded processed text data - Train: 8111, Val: 1432, Test: 2388\n",
      "Feature info: {'tfidf_shape': '(8111, 5000)', 'word2vec_shape': '(8111, 100)', 'mini_sbert_shape': '(8111, 384)', 'train_samples': '8111', 'val_samples': '1432', 'test_samples': '2388', 'class_weights': '{0: 2.2052746057640022, 1: 1.6546307629538963, 2: 0.5148860534501365}'}\n",
      "Loaded word2vec features - Train: (8111, 100), Val: (1432, 100), Test: (2388, 100)\n",
      "Train Accuracy: 0.5859, Train F1: 0.4833\n",
      "Val Accuracy: 0.5594, Val F1: 0.4547\n",
      "Overfitting: 0.0286\n",
      "Results saved to: model_results\\tweet_base\\word2vec\\logreg_simple\n",
      "\n",
      "[6/15] Processing tweet_base with mini_sbert\n",
      "Training LogisticRegression with tweet_base/mini_sbert features\n",
      "Loading data for tweet_base with mini_sbert features\n",
      "Loaded processed text data - Train: 8111, Val: 1432, Test: 2388\n",
      "Feature info: {'tfidf_shape': '(8111, 5000)', 'word2vec_shape': '(8111, 100)', 'mini_sbert_shape': '(8111, 384)', 'train_samples': '8111', 'val_samples': '1432', 'test_samples': '2388', 'class_weights': '{0: 2.2052746057640022, 1: 1.6546307629538963, 2: 0.5148860534501365}'}\n",
      "Loaded mini_sbert features - Train: (8111, 384), Val: (1432, 384), Test: (2388, 384)\n",
      "Train Accuracy: 0.7307, Train F1: 0.6873\n",
      "Val Accuracy: 0.6487, Val F1: 0.5870\n",
      "Overfitting: 0.1003\n",
      "Results saved to: model_results\\tweet_base\\mini_sbert\\logreg_simple\n",
      "\n",
      "[7/15] Processing tweet_porter with tfidf\n",
      "Training LogisticRegression with tweet_porter/tfidf features\n",
      "Loading data for tweet_porter with tfidf features\n",
      "Loaded processed text data - Train: 8111, Val: 1432, Test: 2388\n",
      "Feature info: {'tfidf_shape': '(8111, 5000)', 'word2vec_shape': '(8111, 100)', 'mini_sbert_shape': '(8111, 384)', 'train_samples': '8111', 'val_samples': '1432', 'test_samples': '2388', 'class_weights': '{0: 2.2052746057640022, 1: 1.6546307629538963, 2: 0.5148860534501365}'}\n",
      "Loaded tfidf features - Train: (8111, 5000), Val: (1432, 5000), Test: (2388, 5000)\n",
      "Train Accuracy: 0.8951, Train F1: 0.8715\n",
      "Val Accuracy: 0.7856, Val F1: 0.7251\n",
      "Overfitting: 0.1464\n",
      "Results saved to: model_results\\tweet_porter\\tfidf\\logreg_simple\n",
      "\n",
      "[8/15] Processing tweet_porter with word2vec\n",
      "Training LogisticRegression with tweet_porter/word2vec features\n",
      "Loading data for tweet_porter with word2vec features\n",
      "Loaded processed text data - Train: 8111, Val: 1432, Test: 2388\n",
      "Feature info: {'tfidf_shape': '(8111, 5000)', 'word2vec_shape': '(8111, 100)', 'mini_sbert_shape': '(8111, 384)', 'train_samples': '8111', 'val_samples': '1432', 'test_samples': '2388', 'class_weights': '{0: 2.2052746057640022, 1: 1.6546307629538963, 2: 0.5148860534501365}'}\n",
      "Loaded word2vec features - Train: (8111, 100), Val: (1432, 100), Test: (2388, 100)\n",
      "Train Accuracy: 0.6182, Train F1: 0.5112\n",
      "Val Accuracy: 0.5999, Val F1: 0.4905\n",
      "Overfitting: 0.0207\n",
      "Results saved to: model_results\\tweet_porter\\word2vec\\logreg_simple\n",
      "\n",
      "[9/15] Processing tweet_porter with mini_sbert\n",
      "Training LogisticRegression with tweet_porter/mini_sbert features\n",
      "Loading data for tweet_porter with mini_sbert features\n",
      "Loaded processed text data - Train: 8111, Val: 1432, Test: 2388\n",
      "Feature info: {'tfidf_shape': '(8111, 5000)', 'word2vec_shape': '(8111, 100)', 'mini_sbert_shape': '(8111, 384)', 'train_samples': '8111', 'val_samples': '1432', 'test_samples': '2388', 'class_weights': '{0: 2.2052746057640022, 1: 1.6546307629538963, 2: 0.5148860534501365}'}\n",
      "Loaded mini_sbert features - Train: (8111, 384), Val: (1432, 384), Test: (2388, 384)\n",
      "Train Accuracy: 0.6955, Train F1: 0.6498\n",
      "Val Accuracy: 0.6404, Val F1: 0.5812\n",
      "Overfitting: 0.0686\n",
      "Results saved to: model_results\\tweet_porter\\mini_sbert\\logreg_simple\n",
      "\n",
      "[10/15] Processing whitespace_lancaster with tfidf\n",
      "Training LogisticRegression with whitespace_lancaster/tfidf features\n",
      "Loading data for whitespace_lancaster with tfidf features\n",
      "Loaded processed text data - Train: 8111, Val: 1432, Test: 2388\n",
      "Feature info: {'tfidf_shape': '(8111, 5000)', 'word2vec_shape': '(8111, 100)', 'mini_sbert_shape': '(8111, 384)', 'train_samples': '8111', 'val_samples': '1432', 'test_samples': '2388', 'class_weights': '{0: 2.2052746057640022, 1: 1.6546307629538963, 2: 0.5148860534501365}'}\n",
      "Loaded tfidf features - Train: (8111, 5000), Val: (1432, 5000), Test: (2388, 5000)\n",
      "Train Accuracy: 0.8909, Train F1: 0.8659\n",
      "Val Accuracy: 0.7737, Val F1: 0.7119\n",
      "Overfitting: 0.1540\n",
      "Results saved to: model_results\\whitespace_lancaster\\tfidf\\logreg_simple\n",
      "\n",
      "[11/15] Processing whitespace_lancaster with word2vec\n",
      "Training LogisticRegression with whitespace_lancaster/word2vec features\n",
      "Loading data for whitespace_lancaster with word2vec features\n",
      "Loaded processed text data - Train: 8111, Val: 1432, Test: 2388\n",
      "Feature info: {'tfidf_shape': '(8111, 5000)', 'word2vec_shape': '(8111, 100)', 'mini_sbert_shape': '(8111, 384)', 'train_samples': '8111', 'val_samples': '1432', 'test_samples': '2388', 'class_weights': '{0: 2.2052746057640022, 1: 1.6546307629538963, 2: 0.5148860534501365}'}\n",
      "Loaded word2vec features - Train: (8111, 100), Val: (1432, 100), Test: (2388, 100)\n",
      "Train Accuracy: 0.6060, Train F1: 0.5021\n",
      "Val Accuracy: 0.5992, Val F1: 0.4881\n",
      "Overfitting: 0.0140\n",
      "Results saved to: model_results\\whitespace_lancaster\\word2vec\\logreg_simple\n",
      "\n",
      "[12/15] Processing whitespace_lancaster with mini_sbert\n",
      "Training LogisticRegression with whitespace_lancaster/mini_sbert features\n",
      "Loading data for whitespace_lancaster with mini_sbert features\n",
      "Loaded processed text data - Train: 8111, Val: 1432, Test: 2388\n",
      "Feature info: {'tfidf_shape': '(8111, 5000)', 'word2vec_shape': '(8111, 100)', 'mini_sbert_shape': '(8111, 384)', 'train_samples': '8111', 'val_samples': '1432', 'test_samples': '2388', 'class_weights': '{0: 2.2052746057640022, 1: 1.6546307629538963, 2: 0.5148860534501365}'}\n",
      "Loaded mini_sbert features - Train: (8111, 384), Val: (1432, 384), Test: (2388, 384)\n",
      "Train Accuracy: 0.6845, Train F1: 0.6396\n",
      "Val Accuracy: 0.6236, Val F1: 0.5639\n",
      "Overfitting: 0.0757\n",
      "Results saved to: model_results\\whitespace_lancaster\\mini_sbert\\logreg_simple\n",
      "\n",
      "[13/15] Processing word_lemma with tfidf\n",
      "Training LogisticRegression with word_lemma/tfidf features\n",
      "Loading data for word_lemma with tfidf features\n",
      "Loaded processed text data - Train: 8111, Val: 1432, Test: 2388\n",
      "Feature info: {'tfidf_shape': '(8111, 5000)', 'word2vec_shape': '(8111, 100)', 'mini_sbert_shape': '(8111, 384)', 'train_samples': '8111', 'val_samples': '1432', 'test_samples': '2388', 'class_weights': '{0: 2.2052746057640022, 1: 1.6546307629538963, 2: 0.5148860534501365}'}\n",
      "Loaded tfidf features - Train: (8111, 5000), Val: (1432, 5000), Test: (2388, 5000)\n",
      "Train Accuracy: 0.8941, Train F1: 0.8694\n",
      "Val Accuracy: 0.7884, Val F1: 0.7253\n",
      "Overfitting: 0.1441\n",
      "Results saved to: model_results\\word_lemma\\tfidf\\logreg_simple\n",
      "\n",
      "[14/15] Processing word_lemma with word2vec\n",
      "Training LogisticRegression with word_lemma/word2vec features\n",
      "Loading data for word_lemma with word2vec features\n",
      "Loaded processed text data - Train: 8111, Val: 1432, Test: 2388\n",
      "Feature info: {'tfidf_shape': '(8111, 5000)', 'word2vec_shape': '(8111, 100)', 'mini_sbert_shape': '(8111, 384)', 'train_samples': '8111', 'val_samples': '1432', 'test_samples': '2388', 'class_weights': '{0: 2.2052746057640022, 1: 1.6546307629538963, 2: 0.5148860534501365}'}\n",
      "Loaded word2vec features - Train: (8111, 100), Val: (1432, 100), Test: (2388, 100)\n",
      "Train Accuracy: 0.6070, Train F1: 0.4981\n",
      "Val Accuracy: 0.5873, Val F1: 0.4718\n",
      "Overfitting: 0.0263\n",
      "Results saved to: model_results\\word_lemma\\word2vec\\logreg_simple\n",
      "\n",
      "[15/15] Processing word_lemma with mini_sbert\n",
      "Training LogisticRegression with word_lemma/mini_sbert features\n",
      "Loading data for word_lemma with mini_sbert features\n",
      "Loaded processed text data - Train: 8111, Val: 1432, Test: 2388\n",
      "Feature info: {'tfidf_shape': '(8111, 5000)', 'word2vec_shape': '(8111, 100)', 'mini_sbert_shape': '(8111, 384)', 'train_samples': '8111', 'val_samples': '1432', 'test_samples': '2388', 'class_weights': '{0: 2.2052746057640022, 1: 1.6546307629538963, 2: 0.5148860534501365}'}\n",
      "Loaded mini_sbert features - Train: (8111, 384), Val: (1432, 384), Test: (2388, 384)\n",
      "Train Accuracy: 0.7262, Train F1: 0.6814\n",
      "Val Accuracy: 0.6501, Val F1: 0.5905\n",
      "Overfitting: 0.0909\n",
      "Results saved to: model_results\\word_lemma\\mini_sbert\\logreg_simple\n",
      "\n",
      "Top 5 combinations:\n",
      "           Preprocessing Feature Type  Train Accuracy  Train F1  Val Accuracy  \\\n",
      "0        regexp_snowball        tfidf        0.894711  0.870618      0.788408   \n",
      "12            word_lemma        tfidf        0.894094  0.869424      0.788408   \n",
      "6           tweet_porter        tfidf        0.895081  0.871519      0.785615   \n",
      "9   whitespace_lancaster        tfidf        0.890889  0.865931      0.773743   \n",
      "3             tweet_base        tfidf        0.896190  0.872069      0.770251   \n",
      "\n",
      "      Val F1  Overfitting  \n",
      "0   0.729334     0.141284  \n",
      "12  0.725303     0.144121  \n",
      "6   0.725076     0.146443  \n",
      "9   0.711907     0.154023  \n",
      "3   0.702418     0.169651  \n",
      "BEST MODEL BY Val F1:\n",
      "==================================================\n",
      "Preprocessing: regexp_snowball\n",
      "Feature Type: tfidf\n",
      "Val F1: 0.729334221754586\n",
      "Train F1: 0.8706180358626593\n",
      "Overfitting: 0.14128381410807334\n",
      "\n",
      "All combinations tested!\n"
     ]
    }
   ],
   "source": [
    "# Get all combinations\n",
    "all_combinations = get_all_combinations()\n",
    "total_combinations = len(all_combinations)\n",
    "\n",
    "print(f\"Testing LogisticRegression on all {total_combinations} combinations...\")\n",
    "\n",
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "# Process each combination\n",
    "for i, (preprocessing_version, feature_type) in enumerate(all_combinations):\n",
    "    print(f\"\\n[{i+1}/{total_combinations}] Processing {preprocessing_version} with {feature_type}\")\n",
    "    \n",
    "    # Skip to next combination if error occurs\n",
    "    try:\n",
    "        # Train model\n",
    "        result = train_simple_logreg(preprocessing_version, feature_type)\n",
    "        \n",
    "        # Save results\n",
    "        save_results(result, \"logreg_simple\", preprocessing_version, feature_type)\n",
    "        \n",
    "        # Create log entry\n",
    "        log_entry = {\n",
    "            'Preprocessing': preprocessing_version,\n",
    "            'Feature Type': feature_type,\n",
    "            'Train Accuracy': result['train_accuracy'],\n",
    "            'Train F1': result['train_f1'],\n",
    "            'Val Accuracy': result['val_accuracy'],\n",
    "            'Val F1': result['val_f1'],\n",
    "            'Overfitting': result['overfitting']\n",
    "        }\n",
    "        \n",
    "        # Add to results list\n",
    "        all_results.append(log_entry)\n",
    "        \n",
    "        # Update log file after each model\n",
    "        log_results(all_results, log_file=\"logreg_all_combinations_log.csv\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error with {preprocessing_version}/{feature_type}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d2085f8e-2529-4e58-96d5-8b6a928ef104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All combinations:\n",
      "           Preprocessing Feature Type  Train Accuracy  Train F1  Val Accuracy  \\\n",
      "10  whitespace_lancaster     word2vec        0.605967  0.502069      0.599162   \n",
      "7           tweet_porter     word2vec        0.618173  0.511181      0.599860   \n",
      "13            word_lemma     word2vec        0.606954  0.498093      0.587291   \n",
      "4             tweet_base     word2vec        0.585871  0.483259      0.559358   \n",
      "1        regexp_snowball     word2vec        0.610652  0.506941      0.585894   \n",
      "8           tweet_porter   mini_sbert        0.695475  0.649829      0.640363   \n",
      "2        regexp_snowball   mini_sbert        0.701147  0.655663      0.642458   \n",
      "11  whitespace_lancaster   mini_sbert        0.684503  0.639602      0.623603   \n",
      "14            word_lemma   mini_sbert        0.726174  0.681396      0.650140   \n",
      "5             tweet_base   mini_sbert        0.730736  0.687321      0.648743   \n",
      "0        regexp_snowball        tfidf        0.894711  0.870618      0.788408   \n",
      "12            word_lemma        tfidf        0.894094  0.869424      0.788408   \n",
      "6           tweet_porter        tfidf        0.895081  0.871519      0.785615   \n",
      "9   whitespace_lancaster        tfidf        0.890889  0.865931      0.773743   \n",
      "3             tweet_base        tfidf        0.896190  0.872069      0.770251   \n",
      "\n",
      "      Val F1  Overfitting  \n",
      "10  0.488102     0.013967  \n",
      "7   0.490473     0.020708  \n",
      "13  0.471838     0.026255  \n",
      "4   0.454705     0.028554  \n",
      "1   0.476607     0.030334  \n",
      "8   0.581202     0.068627  \n",
      "2   0.585032     0.070631  \n",
      "11  0.563869     0.075733  \n",
      "14  0.590518     0.090878  \n",
      "5   0.587022     0.100299  \n",
      "0   0.729334     0.141284  \n",
      "12  0.725303     0.144121  \n",
      "6   0.725076     0.146443  \n",
      "9   0.711907     0.154023  \n",
      "3   0.702418     0.169651  \n",
      "BEST MODEL BY Val F1:\n",
      "==================================================\n",
      "Preprocessing: regexp_snowball\n",
      "Feature Type: tfidf\n",
      "Val F1: 0.729334221754586\n",
      "Train F1: 0.8706180358626593\n",
      "Overfitting: 0.14128381410807334\n",
      "\n",
      "All combinations tested!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create a DataFrame with all results\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Sort by validation F1 score\n",
    "if not results_df.empty:\n",
    "    results_df = results_df.sort_values('Overfitting', ascending=True)\n",
    "    results_df.to_csv('logreg_all_combinations_results.csv', index=False)\n",
    "    \n",
    "    # Display top results\n",
    "    print(\"\\nAll combinations:\")\n",
    "    print(results_df)\n",
    "    \n",
    "    # Create visualizations comparing results\n",
    "    plot_model_comparison(\n",
    "        results_df, \n",
    "        metric='Val F1', \n",
    "        group_by='Preprocessing', \n",
    "        hue='Feature Type',\n",
    "        title='Validation F1 Score by Preprocessing and Feature Type',\n",
    "        output_path='logreg_all_comparison.png'\n",
    "    )\n",
    "    \n",
    "    plot_overfitting_analysis(\n",
    "        results_df, \n",
    "        group_by='Feature Type',\n",
    "        title='Overfitting by Feature Type',\n",
    "        output_path='logreg_all_overfitting.png'\n",
    "    )\n",
    "    \n",
    "    # Find the best model\n",
    "    find_best_model(\n",
    "        results_df, \n",
    "        metric='Val F1',\n",
    "        min_columns=['Preprocessing', 'Feature Type', 'Val F1', 'Train F1', 'Overfitting']\n",
    "    )\n",
    "\n",
    "print(\"\\nAll combinations tested!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70adf7ea-de2a-46fe-a344-ac03e1e7aa14",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (nlp)",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
